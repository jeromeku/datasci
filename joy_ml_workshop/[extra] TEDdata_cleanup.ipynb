{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "TED Talks transcript translation dataset"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Load data by line"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "iprint = False\n",
      "dPath = './ted_mini/art_positive/'\n",
      "flist = [ff for ff in os.listdir(dPath) if ff.endswith('.ted')]\n",
      "\n",
      "f = flist[0]\n",
      "with open('%s/%s' % (dPath, f), 'rb') as g: \n",
      "    dat = g.readlines()\n",
      "    \n",
      "# dat\n",
      "print f"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10.ted\n"
       ]
      }
     ],
     "prompt_number": 265
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Text Processing: Tokenization, Dropping Stop Words, Stemming and Lemmatization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Magic 1: from paragraph to bag of words using Python string processing only\n",
      "from itertools import chain\n",
      "flat_dat = [w for w in list\n",
      "            (\n",
      "                chain.from_iterable\n",
      "                (\n",
      "                    [\n",
      "                        [''.join(re.findall(r'<a.*?/a>|<[^\\>]*>|[\\w\\\"@#]+', \n",
      "                                        d.replace(\"_en\", \"\").strip(\"\\n\").lower())) \n",
      "                         for d in l.split(' ') if d not in my_stop_words\n",
      "                         ] for l in dat\n",
      "                    ]\n",
      "                )\n",
      "            ) \n",
      "            if w not in my_stop_words and len(w) > 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 213
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Breaking it down\n",
      "# 1. List of list of clean words [d.replace(\"_en\", \"\").strip(\"\\n\") for d in l.split(' ')]\n",
      "line1 = dat[0]\n",
      "wordlist1 = line1.split(' ')\n",
      "word1 = wordlist1[0]\n",
      "cleanword1 = [w.lower().replace(\"_en\", \"\").strip(\"\\n\").lower() for w in wordlist1]\n",
      "cleanword1_nopunc = [''.join(re.findall(r'<a.*?/a>|<[^\\>]*>|[\\w\\\"@#]+', w)) for w in cleanword1]\n",
      "my_stop_words = ['i', 'me', 'my', 'myself',\n",
      "                 'we', 'our', 'ours', 'ourselves',\n",
      "                 'you', 'your', 'yours', 'yourself', 'yourselves',\n",
      "                 'he', 'him', 'his', 'himself',\n",
      "                 'she', 'her', 'hers', 'herself',\n",
      "                 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
      "                 'what', 'which', 'who', 'whom', \n",
      "                 'this', 'that', 'these', 'those',\n",
      "                 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
      "                 'have', 'has', 'had', 'having',\n",
      "                 'do', 'does', 'did', 'doing',\n",
      "                 'a', 'an', 'the', 'and',\n",
      "                 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
      "                 'between', 'into', 'through', 'during', 'before', 'after', \n",
      "                 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', \n",
      "                 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', \n",
      "                 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
      "                 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than',\n",
      "                 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now',\n",
      "                 '-', ':', '.', '\\'', '\\',', ',', '#', '/', '@', '.,', '(', ')', 'RT', 'I', 'I''m', '\"']\n",
      "\n",
      "cleanword_without_stop_words1 = [w for w in cleanword1_nopunc if (w not in my_stop_words) & (len(w) > 0)]\n",
      "\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "lmtzr = WordNetLemmatizer()\n",
      "lemmatized_words1 = [lmtzr.lemmatize(w).lower() for w in cleanword_without_stop_words1]\n",
      "\n",
      "if iprint: \n",
      "    print \"entire raw document:\\n %s \\n\" % dat\n",
      "    print \"line 1:\\n %s \\n\" % line1\n",
      "    print \"wordlist 1:\\n %s \\n\" % wordlist1\n",
      "    print \"word 1:\\n %s \\n\" % word1\n",
      "    print \"clean word 1:\\n %s\\n\" % cleanword1\n",
      "    print \"clean word no punctuation 1:\\n %s\\n\" % cleanword1_nopunc\n",
      "    print \"lemmatized words 1:\\n %s\" % lemmatized_words1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 214
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Magic 2. FlatMaping list of list of cleanwords into a single list\n",
      "import string\n",
      "import re\n",
      "flatstr = ''.join(''.join([d.replace(\"_en\", \"\").strip(\"\\n\") for d in dat]))\n",
      "# flatstr_nopunc1 = ' '.join(re.findall(r'<a.*?/a>|<[^\\>]*>|[\\w\\\"@]+', flatstr))\n",
      "flatstr_nopunc1 = ' '.join(re.findall(r'\\w+', flatstr))\n",
      "flatstr_nopunc2 = re.sub('[{}]'.format(re.escape(string.punctuation)), '', flatstr)\n",
      "\n",
      "flat_dat = lemma_tokenize(flatstr_nopunc1)\n",
      "# # Remove digit \n",
      "flat_dat_nodigit = filter(lambda x: not x.isdigit(), lemma_tokenize(flatstr_nopunc))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 282
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "lmtzr = WordNetLemmatizer()\n",
      "paragraph = ''.join(''.join([d.replace(\"_en\", \"\").strip(\"\\n\") for d in dat]))\n",
      "sentence2 = detector.tokenize(paragraph.strip())[0]\n",
      "words2 = nltk.tokenize.word_tokenize(sentence0)\n",
      "stopwords =  nltk.corpus.stopwords.words('english')\n",
      "words_without_stopwords2 = [w for w in words if w not in stopwords]\n",
      "lemmatized_words2 = [lmtzr.lemmatize(w).lower() for w in words_without_stopwords]\n",
      "if iprint: \n",
      "    print \"paragraph:\\n %s\\n\" % paragraph\n",
      "    print \"sentence0:\\n %s\\n\" % sentence2\n",
      "    print \"words:\\n %s\\n\" % words2\n",
      "    print \"words without stopwords:\\n %s\\n\" % words_without_stopwords2\n",
      "    print \"lemmatized words:\\n %s \\n\" % lemmatized_words2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 216
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Functions for doing the Magic in one step"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import nltk.tokenize\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.tokenize.api import StringTokenizer\n",
      "import numpy as np\n",
      "import numpy.linalg\n",
      "import sys\n",
      "\n",
      "def stopwords():\n",
      "    try:\n",
      "        stop_words = stopwords.stop_words\n",
      "    except AttributeError:\n",
      "        try:\n",
      "            stop_words = nltk.corpus.stopwords.words('english')\n",
      "        except LookupError:\n",
      "            nltk.download('stopwords')\n",
      "            stop_words = nltk.corpus.stopwords.words('english')\n",
      "            stop_words.extend(['-', ':', '.', '\\'', '\\',', ',', '#', '/', '@', '.,', '(', ')', 'RT', 'I', 'I''m'])\n",
      "            stopwords.stop_words = stop_words\n",
      "    return stop_words\n",
      "\n",
      "def text_to_bagsofword(dat):\n",
      "    flatstr = ''.join(''.join([d.replace(\"_en\", \"\").strip(\"\\n\") for d in dat]))\n",
      "    flatstr_nopunc = ' '.join(re.findall(r'<a.*?/a>|<[^\\>]*>|[\\w\\\"@#]+', flatstr))\n",
      "    return lemma_tokenize(flatstr_nopunc)\n",
      "\n",
      "def lemma_tokenize(paragraph):\n",
      "    lmtzr = WordNetLemmatizer()\n",
      "    try:\n",
      "        return [lmtzr.lemmatize(word).lower() for sentence in tokenize(paragraph) for word in sentence]\n",
      "    except LookupError:\n",
      "        nltk.download('wordnet')\n",
      "        return [lmtzr.lemmatize(word).lower() for sentence in tokenize(paragraph) for word in sentence]\n",
      "    \n",
      "def tokenize(paragraph):\n",
      "    if not paragraph:\n",
      "        return []\n",
      "    try:\n",
      "        detector = tokenize.detector\n",
      "    except AttributeError:\n",
      "        try:\n",
      "            detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "        except LookupError:\n",
      "            nltk.download('punkt')\n",
      "            detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "            tokenize.detector = detector\n",
      "    return [\n",
      "                [\n",
      "                    word\n",
      "                    for word in nltk.tokenize.word_tokenize(sentence)\n",
      "                    if word not in stopwords()\n",
      "                ]\n",
      "                for sentence in detector.tokenize(paragraph.strip())\n",
      "            ]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 262
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Term Frequency\n",
      "flat_dat = text_to_bagsofword(dat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 263
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 264
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 211
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}