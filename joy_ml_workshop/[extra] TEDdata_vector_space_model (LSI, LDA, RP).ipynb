{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import nltk.tokenize\n",
      "from nltk.stem.wordnet import WordNetLemmatizer\n",
      "from nltk.tokenize.api import StringTokenizer\n",
      "import numpy as np\n",
      "import numpy.linalg\n",
      "import sys\n",
      "import re\n",
      "\n",
      "def stopwords():\n",
      "    try:\n",
      "        stop_words = stopwords.stop_words\n",
      "    except AttributeError:\n",
      "        try:\n",
      "            stop_words = nltk.corpus.stopwords.words('english')\n",
      "        except LookupError:\n",
      "            nltk.download('stopwords')\n",
      "            stop_words = nltk.corpus.stopwords.words('english')\n",
      "            stop_words.extend(['-', ':', '.', '\\'', '\\',', ',', '#', '/', '@', '.,', '(', ')', 'RT', 'I', 'I''m'])\n",
      "            stopwords.stop_words = stop_words\n",
      "    return stop_words\n",
      "\n",
      "def text_to_bagsofword(dat):\n",
      "    flatstr = ''.join(''.join([d.replace(\"_en\", \"\").strip(\"\\n\") for d in dat]))\n",
      "    flatstr_nopunc = ' '.join(re.findall(r'<a.*?/a>|<[^\\>]*>|[\\w\\\"@#]+', flatstr))\n",
      "    return lemma_tokenize(flatstr_nopunc)\n",
      "\n",
      "def lemma_tokenize(paragraph):\n",
      "    lmtzr = WordNetLemmatizer()\n",
      "    try:\n",
      "        return [lmtzr.lemmatize(word).lower() for sentence in tokenize(paragraph) for word in sentence]\n",
      "    except LookupError:\n",
      "        nltk.download('wordnet')\n",
      "        return [lmtzr.lemmatize(word).lower() for sentence in tokenize(paragraph) for word in sentence]\n",
      "    \n",
      "def tokenize(paragraph):\n",
      "    if not paragraph:\n",
      "        return []\n",
      "    try:\n",
      "        detector = tokenize.detector\n",
      "    except AttributeError:\n",
      "        try:\n",
      "            detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "        except LookupError:\n",
      "            nltk.download('punkt')\n",
      "            detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "            tokenize.detector = detector\n",
      "    return [\n",
      "                [\n",
      "                    word\n",
      "                    for word in nltk.tokenize.word_tokenize(sentence)\n",
      "                    if word not in stopwords()\n",
      "                ]\n",
      "                for sentence in detector.tokenize(paragraph.strip())\n",
      "            ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import corpora, models, similarities\n",
      "import os\n",
      "art_dPath = './ted_mini/art_positive'\n",
      "falist = [ffa for ffa in os.listdir(art_dPath) if ffa.endswith('.ted')]\n",
      "art_corpus = []\n",
      "for fa in falist: \n",
      "    with open('%s/%s' % (art_dPath, fa), 'rb') as ga: \n",
      "#         print \"art transcript :%s\" % fa\n",
      "        art_corpus.append(text_to_bagsofword(ga.readlines()))\n",
      "    \n",
      "dictionary = corpora.Dictionary(art_corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_doc = \"Human computer interaction\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_vec = dictionary.doc2bow(new_doc.lower().split())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(new_vec)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(14, 1), (315, 1), (6589, 1)]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "art_corp = [dictionary.doc2bow(text) for text in art_corpus]\n",
      "corpora.MmCorpus.serialize('/tmp/art_corpus.mm', art_corp)\n",
      "tfidf = models.TfidfModel(art_corp)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_tfidf = tfidf[art_corp]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)\n",
      "corpus_lsi = lsi[corpus_tfidf]\n",
      "lsi.print_topics(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "[u'-0.086*\"said\" + -0.085*\"design\" + -0.083*\"car\" + -0.076*\"music\" + -0.075*\"story\" + -0.071*\"oil\" + -0.064*\"piece\" + -0.062*\"machine\" + -0.059*\"know\" + -0.058*\"computer\"',\n",
        " u'-0.161*\"origami\" + -0.133*\"structure\" + -0.125*\"fold\" + -0.123*\"dna\" + -0.113*\"design\" + -0.111*\"strand\" + -0.099*\"spell\" + -0.098*\"machine\" + -0.098*\"pattern\" + 0.094*\"vagina\"']"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsi.print_topics(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "[u'-0.086*\"said\" + -0.085*\"design\" + -0.083*\"car\" + -0.076*\"music\" + -0.075*\"story\" + -0.071*\"oil\" + -0.064*\"piece\" + -0.062*\"machine\" + -0.059*\"know\" + -0.058*\"computer\"']"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lsi.print_topics(4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "[u'-0.086*\"said\" + -0.085*\"design\" + -0.083*\"car\" + -0.076*\"music\" + -0.075*\"story\" + -0.071*\"oil\" + -0.064*\"piece\" + -0.062*\"machine\" + -0.059*\"know\" + -0.058*\"computer\"',\n",
        " u'-0.161*\"origami\" + -0.133*\"structure\" + -0.125*\"fold\" + -0.123*\"dna\" + -0.113*\"design\" + -0.111*\"strand\" + -0.099*\"spell\" + -0.098*\"machine\" + -0.098*\"pattern\" + 0.094*\"vagina\"']"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus_tfidf"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<gensim.interfaces.TransformedCorpus object at 0x113e6a5d0>\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}