{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Exercise: From one document to corpus"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Exercise 3.1: We have TED transcipts from art ('./ted_mini/art_positive/') and science ('./ted_mini/sci_positive/') talks. Let's build corpus of text from these transcripts to train a classifier.\n",
      "\n",
      "1. Use os module to get the list of all transcript files in each folder\n",
      "os.listdir returns list of objects in current directory\n",
      "2. Read each transcript file, tokenize raw text into word list\n",
      "3. Append the words list into list of list (that's a representation of corpus)\n",
      "4. (Pickle) Save corpora to file using pickle module for later use"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ## Functions below turn raw text into bag of words\n",
      "# ## Feel free to uncomment and have fun with it!!\n",
      "# ---------------------------------------------\n",
      "# import nltk\n",
      "# import nltk.tokenize\n",
      "# from nltk.stem.wordnet import WordNetLemmatizer\n",
      "# from nltk.tokenize.api import StringTokenizer\n",
      "# import numpy as np\n",
      "# import numpy.linalg\n",
      "# import sys\n",
      "# import re\n",
      "\n",
      "# def stopwords():\n",
      "#     try:\n",
      "#         stop_words = stopwords.stop_words\n",
      "#     except AttributeError:\n",
      "#         try:\n",
      "#             stop_words = nltk.corpus.stopwords.words('english')\n",
      "#         except LookupError:\n",
      "#             nltk.download('stopwords')\n",
      "#             stop_words = nltk.corpus.stopwords.words('english')\n",
      "#             stop_words.extend(['-', ':', '.', '\\'', '\\',', ',', '#', '/', '@', '.,', '(', ')', 'RT', 'I', 'I''m'])\n",
      "#             stopwords.stop_words = stop_words\n",
      "#     return stop_words\n",
      "\n",
      "# def text_to_bagsofword(dat):\n",
      "#     flatstr = ''.join(''.join([d.replace(\"_en\", \"\").strip(\"\\n\") for d in dat]))\n",
      "#     flatstr_nopunc = ' '.join(re.findall(r'\\w+', flatstr))\n",
      "#     return lemma_tokenize(flatstr_nopunc)\n",
      "\n",
      "# def lemma_tokenize(paragraph):\n",
      "#     lmtzr = WordNetLemmatizer()\n",
      "#     try:\n",
      "#         return [lmtzr.lemmatize(word).lower() for sentence in tokenize(paragraph) for word in sentence]\n",
      "#     except LookupError:\n",
      "#         nltk.download('wordnet')\n",
      "#         return [lmtzr.lemmatize(word).lower() for sentence in tokenize(paragraph) for word in sentence]\n",
      "    \n",
      "# def tokenize(paragraph):\n",
      "#     if not paragraph:\n",
      "#         return []\n",
      "#     try:\n",
      "#         detector = tokenize.detector\n",
      "#     except AttributeError:\n",
      "#         try:\n",
      "#             detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "#         except LookupError:\n",
      "#             nltk.download('punkt')\n",
      "#             detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "#             tokenize.detector = detector\n",
      "#     return [\n",
      "#                 [\n",
      "#                     word\n",
      "#                     for word in nltk.tokenize.word_tokenize(sentence)\n",
      "#                     if word not in stopwords() and len(word) > 1 and not word.isdigit()\n",
      "#                 ]\n",
      "#                 for sentence in detector.tokenize(paragraph.strip())\n",
      "#             ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Training Naive Bayes Model"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "NLTK Module comes with builtin corpus\n",
      "Here's an example of training Naive Bayes Model using builtin corpus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Example Naive Bayes Classifier using builtin dataset\n",
      "from nltk import NaiveBayesClassifier\n",
      "import nltk.classify\n",
      "from nltk.corpus import movie_reviews as mr\n",
      "\n",
      "def feats(words):\n",
      "    return dict([(w, True) for w in words])\n",
      "\n",
      "negids = mr.fileids('neg')\n",
      "posids = mr.fileids('pos')\n",
      "\n",
      "negfeats = [(feats(mr.words(fileids=[f])), 'neg') for f in negids[:50]]\n",
      "posfeats = [(feats(mr.words(fileids=[f])), 'pos') for f in posids[:50]]\n",
      "\n",
      "trainfeats = negfeats[:40] + posfeats[:40]\n",
      "testfeats = negfeats[40:] + posfeats[40:]\n",
      "\n",
      "classifier = NaiveBayesClassifier.train(trainfeats)\n",
      "print \"accurary: \", nltk.classify.util.accuracy(classifier, testfeats)\n",
      "classifier.show_most_informative_features(n=20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "accurary:  0.6\n",
        "Most Informative Features\n",
        "                    fine = True              neg : pos    =      7.0 : 1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "                   worst = True              neg : pos    =      6.3 : 1.0\n",
        "                    whom = True              pos : neg    =      5.7 : 1.0\n",
        "                  unlike = True              pos : neg    =      5.7 : 1.0\n",
        "              supporting = True              pos : neg    =      5.7 : 1.0\n",
        "                    word = True              pos : neg    =      5.0 : 1.0\n",
        "                 talking = True              neg : pos    =      5.0 : 1.0\n",
        "                  boring = True              neg : pos    =      5.0 : 1.0\n",
        "               enjoyable = True              pos : neg    =      5.0 : 1.0\n",
        "                 general = True              pos : neg    =      5.0 : 1.0\n",
        "                    mess = True              neg : pos    =      5.0 : 1.0\n",
        "            performances = True              pos : neg    =      5.0 : 1.0\n",
        "                    hour = True              neg : pos    =      4.6 : 1.0\n",
        "               excellent = True              pos : neg    =      4.3 : 1.0\n",
        "                     joe = True              pos : neg    =      4.3 : 1.0\n",
        "                      de = True              pos : neg    =      4.3 : 1.0\n",
        "                thriller = True              neg : pos    =      4.3 : 1.0\n",
        "                 quality = True              pos : neg    =      4.3 : 1.0\n",
        "                    rock = True              pos : neg    =      4.3 : 1.0\n",
        "                   faces = True              pos : neg    =      4.3 : 1.0\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Exercise 3.2: Now is your turn! Use the `art_corpus` and `sci_corpus` we built above, train a Naive Bayes classifier\n",
      "What are the 50 most informative features (tokens) for discriminating art vs. science TED talks?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}