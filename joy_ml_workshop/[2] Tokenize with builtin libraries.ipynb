{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Exercise: Tokenize with NLTK tokenizer or sklearn tokenizer"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Exercise 2.1 Load text data in '1.ted' from from './ted_mini/sci_positive/' directory.\n",
      "Clean up text then tokenize with NLTK tokenizer or sklearn tokenizer\n",
      "\n",
      "From last lecture: CountVectorizer from sklearn\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "Other libraries: Gensim\n",
      "from gensim import corpora"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Hint: You can use readline() or readlines() to read text file\n",
      "# Then use sci-kit learn CountVectorizer (last lecture) to tokenize/vectorize\n",
      "# from sklearn.feature_extraction.text import CountVectorizer\n",
      "#\n",
      "# ## Feel free to uncomment the code below to turn raw data into bag of words \n",
      "# -------------------------------------------\n",
      "# from nltk.stem.wordnet import WordNetLemmatizer\n",
      "# from nltk.stem import PorterStemmer\n",
      "# import nltk\n",
      "# import re, string\n",
      "\n",
      "## Read in raw data\n",
      "# dPath = './ted_mini/sci_positive/'\n",
      "# with open('%s/%s' % (dPath, '1.ted'), 'r') as f: \n",
      "#     dat = f.readlines()\n",
      "\n",
      "## Clean/Tokenize/Stemmize&Lemmatize\n",
      "# dat_list = list(chain(*map(lambda x: x.replace(\"_en\", \"\").strip(\"\\n\").lower().split(' '), dat)))\n",
      "# dat_list = filter(lambda y: (len(y) > 1) and (not y.isdigit()),\n",
      "#                   map(lambda x: re.sub('[{}]'.format(re.escape(string.punctuation)), '', x), dat_list))\n",
      "# lmtzr = WordNetLemmatizer()\n",
      "# pst = PorterStemmer()\n",
      "# stop_words = nltk.corpus.stopwords.words('english')\n",
      "# # dat_list = filter(lambda y: y not in stop_words, map(lambda x: pst.stem(lmtzr.lemmatize(x)), dat_list))\n",
      "# dat_list = filter(lambda y: y not in stop_words, map(lambda x: lmtzr.lemmatize(x), dat_list))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}